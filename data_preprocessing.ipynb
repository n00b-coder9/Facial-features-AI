{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acting-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "destroyed-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imutils\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "creative-concept",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\faiz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upper-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Haar Cascade\n",
    "## Taken from https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "color-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the function to extract the face \n",
    "def face_extractor(origin, destination, fc):\n",
    "    ## Importing image using open cv\n",
    "    img = cv2.imread(origin,1)\n",
    "\n",
    "    ## Resizing to constant width\n",
    "    img = imutils.resize(img, width=200)\n",
    "    \n",
    "    ## Finding actual size of image\n",
    "    H,W,_ = img.shape\n",
    "    \n",
    "    ## Converting BGR to RGB\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    ## Detecting faces on the image\n",
    "    face_coord = fc.detectMultiScale(gray,1.2,10,minSize=(50,50))\n",
    "    \n",
    "    ## If only one face is found\n",
    "    if len(face_coord) == 1:\n",
    "        X, Y, w, h = face_coord[0]\n",
    "    \n",
    "    ## If no face found --> SKIP\n",
    "    elif len(face_coord)==0:\n",
    "        return None\n",
    "    \n",
    "    ## If multiple faces are found take the one with largest area\n",
    "    else:\n",
    "        max_val = 0\n",
    "        max_idx = 0\n",
    "        for idx in range(len(face_coord)):\n",
    "            _, _, w_i, h_i = face_coord[idx]\n",
    "            if w_i*h_i > max_val:\n",
    "                max_idx = idx\n",
    "                max_val = w_i*h_i\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            X, Y, w, h = face_coord[max_idx]\n",
    "    \n",
    "    ## Crop and export the image\n",
    "    img_cp = img[\n",
    "            max(0,Y - int(0.35*h)): min(Y + int(1.35*h), H),\n",
    "            max(0,X - int(w*0.35)): min(X + int(1.35*w), W)\n",
    "        ].copy()\n",
    "    cv2.imwrite(destination, img_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "official-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n"
     ]
    }
   ],
   "source": [
    "## Defining destination path\n",
    "path = r'D:/tutors_point/project_work/data/faces/'\n",
    "\n",
    "## Finding all the images in the folder\n",
    "item_list = glob.glob('./data/first_50000/*.jpg')\n",
    "print(len(item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "addressed-scout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea35fbbf61e4d92a25afdeb955f72d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for org in tqdm(item_list):\n",
    "    image=cv2.imread(org);\n",
    "    if type(image) is np.ndarray:\n",
    "        face_extractor(origin = org, destination = path+org.split('\\\\')[-1], fc=face_cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "intense-creator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce31801df32407da547234d9541621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Finding all the images and separating in training and validation\n",
    "item_list = glob.glob(path+'*.jpg')\n",
    "for idx,item in zip(tqdm(range(1,1180)),item_list):\n",
    "    if idx <= 700:\n",
    "        destination = 'training/'\n",
    "    else:\n",
    "        destination = 'validation/'\n",
    "        shutil.move(\n",
    "            item.split('\\\\')[0]+'/'+item.split('\\\\')[1], \n",
    "            item.split('\\\\')[0]+'/'+destination+item.split('\\\\')[1]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
